## A Needle in A Haystack

The corresponding Apache log file in this directory has numerous entries, some of which are malicious.
It's all too common to run across log files where you'll have to slice and dice a text file to parse out valuable data.
Sometimes you'll have a nice SIEM to help you query said data, othertimes you'll have to manually parse through logs.
For your first lab, you're tasked to write a script to do the following in the language of your choice:

1. Identify unique count of IPs in this file.
2. Parse out requested URIs with IPs.
3. Perform lookups against a 3rd party API for data enrichment. 
* Is there any historical threat intel?


### Walkthrough Examples
<br />
<details><summary> Bash Example of Unique Count of IPs</summary>

```bash
$> cat access.log  | cut -f 1 -d ' ' | uniq -c 
```
</details>

<details><summary> Bash Example of unique requests associated w/ IPs</summary>

```bash
$> cat access.log  | cut -f 1,7,8,9,10 -d ' '  | uniq -c
```
</details>

<details><summary> Bash Example of Querying API </summary>

```bash
# Get unique IPs
uniqueips=$(cat access.log  | cut -f 1 -d ' ' | uniq -c | awk '{print $2}') # create a list of space separated IPs

# query 
for ip in $uniquieips; do
    results=$(curl https://pulsedive.com/api/explore.php?q=$ip) 
    score=$(echo $results | jq '.results|.[].risk')
    countrycode=$(echo $results  | jq '.results|.[].summary.properties.geo.countrycode')
    echo -e "[$(date -Im)] $ip ($countrycode): $score"
    sleep 10; # be friendly to pulsedive they're awesome!
done
```
</details>


## Hunting in A SIEM
Elasticsearch is an Open Source NoSQL database that is commonly used in industry for log management.
Commerical features support SIEM/Security functionality as well as observability and more.
Kibana is the user interface that queries Elasticsearch for logs. In this scenario we'll setup an Elasticsearch
cluster via docker-compose and upload some log files to begin analyzing the Apache log in a SIEM like environment.


The docker-compose.yml in this directory was taken from the official Elasticsearch documentation and can be [found here](https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html).

To start the Elasticsearch cluster, execute the following:
``` sh
$> docker-compose up -d
```

The Vagrant file performs port forwarding from the host on 5601 to the guest on port 5601 the default Kibana port.
Check that the containers are successfully up and running via ```docker ps```. If everything is up and running, you should be able to access the cluster.
```
vagrant@bsidesroc:~$ docker ps
CONTAINER ID   IMAGE                                                 COMMAND                  CREATED          STATUS                    PORTS                              NAMES
b02a4f434a7a   docker.elastic.co/kibana/kibana:8.1.0                 "/bin/tini -- /usr/l…"   19 minutes ago   Up 19 minutes (healthy)   0.0.0.0:5601->5601/tcp             01_log_analysis_kibana_1
fb91c3578d62   docker.elastic.co/elasticsearch/elasticsearch:8.1.0   "/bin/tini -- /usr/l…"   22 minutes ago   Up 20 minutes (healthy)   9200/tcp, 9300/tcp                 01_log_analysis_es03_1
01720278b636   docker.elastic.co/elasticsearch/elasticsearch:8.1.0   "/bin/tini -- /usr/l…"   22 minutes ago   Up 20 minutes (healthy)   9200/tcp, 9300/tcp                 01_log_analysis_es02_1
7cc1383a1461   docker.elastic.co/elasticsearch/elasticsearch:8.1.0   "/bin/tini -- /usr/l…"   22 minutes ago   Up 20 minutes (healthy)   0.0.0.0:9200->9200/tcp, 9300/tcp   01_log_analysis_es01_1
```

The images below show the process of logging in and uploading the apache.log file in this directory.

First login with the default credentials of elastic/bsidesroc. Note, the credentials are stored in a file in this directgory of ".env". The docker-compose.yml file leverages [.env](https://docs.docker.com/compose/environment-variables/) for filling out environment variables.

![elk-login](./.imgs/00-elk-login.png)

Next, upload the access.log file.
![upload-a-file](./.imgs/01-upload-a-file.png)

Elasticsearch is familiar with common log formats and will try to match underlying fields to their specific data type.
For example, the field "client ip" is matched as [data type](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-types.html) IP and not as a string. This gives us flexibility in the situation we were querying for a specific CIDR block. 

![elk-login](./.imgs/02-import-stats.png)
![import-data](./.imgs/03-import-data.png)
![view-data](./.imgs/04-view-data.png)
This course was ran in mid-March of 2022, the timestamps of the Apache access log reflect this. Update the time period appropriate to when your viewing the course.
![view-range-of-data](./.imgs/05-view-range-of-data.png)

As we saw with manually parsing data, some URIs had wget/curl entries which appeared to be asscoiated with exploit attempts. Filtering on wget reveals some interesting requests. Is there anything that stands out that we should hunt for?

![filter](./.imgs/06-filter-on-specific-values.png)


When you're done, shutdown the ELK instance.

```
$> docker-compose down
```

